business
{ By Jemma Carr For Mailonline   Published:  17:29, 24 February 2020   https://www.dailymail.co.uk//news/article-8038701/Met-Police-chief-Cressida-Dick-defends-facial-recognition-technology.html }
The Met Police Commissioner has defended the use of facial recognition technology  to catch criminals, claiming that privacy concerns are 'much smaller' than protecting the public from 'a knife through the chest'.  Dame Cressida Dick told a conference in Whitehall that 'ill-informed' critics would need to justify the use of the controversial cameras to victims of violent crime who have been caused 'real harm'. The technology - which has been criticised by civil liberties and privacy campaigners - scan members of the public against a 'watchlist', said to contain suspects wanted by police.  The Met claims that the technology has a very low failure rate, with the system only creating a false alert one in every 1,000 times. But the new police cameras have already been branded a failure after spotting no suspects despite scanning 4,600 shoppers in an east London mall. Ms Dick today said it was not the force's job to decide 'where the boundary lies between security and privacy', and argued that the threshold of privacy is likely to have been eroded in the age of social media.  She told delegates at the Royal United Services Institute: 'It is for critics to justify to the victims of those crimes why police should not use tech lawfully and proportionally to catch criminals who caused the victims real harm. 'It is not for me and the police to decide where the boundary lies between security and privacy, though I do think it is right for us to contribute to the debate. 'But speaking as a member of public, I will be frank. In an age of Twitter and Instagram and Facebook, concern about my image and that of my fellow law-abiding citizens passing through LFR (live facial recognition) and not being stored, feels much, much, much smaller than my and the public's vital expectation to be kept safe from a knife through the chest.'    The police chief said that if artificial intelligence could help identify potential terrorists, rapists or killers, most members of the public would want them to use it. Dame Cressida said: 'If, as seems likely, algorithms can assist in identifying patterns of behaviour by those under authorised surveillance, that would otherwise have been missed, patterns that indicate they are radicalising others or are likely to mount a terrorist attack. 'If an algorithm can help identify in our criminal systems material a potential serial rapist or killer that we could not have found by human endeavour alone. 'If a machine can improve our ability to disclose fairly then I think almost all citizens would want us to use it. 'The only people who benefit from us not using lawfully and proportionately tech are the criminals, the rapists, the terrorists and all those who want to harm you, your family and friends.' New police face recognition cameras have been branded a failure after spotting no suspects in hours of surveillance. The UK's biggest force was accused of wasting taxpayer cash after the devices failed to find a single match despite scanning 4,600 shoppers in an east London mall. The Scotland Yard cameras record the faces of passers-by and send an alert to an officer's phone if they look like those on a 'most wanted' list. Human rights groups say they lead to 'false positives' which can mean innocent members of the public being stopped, searched and even arrested. Now it has emerged that the first time the cameras were put to use, at Stratford Centre on February 11, no alerts were sent out. This is despite the fact the devices were switched on for five and a half hours and given a list of 5,816 suspects likely to be in the area at that time. Recent Metropolitan Police use of facial recognition led to the arrest of eight criminals who would not otherwise have been caught, delegates heard. But use of the technology has been criticised as a violation of privacy. Human rights groups say they lead to 'false positives' which can mean innocent members of the public being stopped, searched and even arrested.  Silkie Carlo, from civil liberties group Big Brother Watch, said the cameras 'erode the trust between the police and the public'. He said: 'It is purely magical thinking to suggest facial recognition can solve London's problem with knife crime.  'It's a highly controversial mass surveillance tool. It seriously risks eroding trust between the police and the public. 'The commissioner is right that the loudest voices in this debate are the critics, it's just that she's not willing to listen to them.  'Her attempt to dismiss serious human rights concerns with life or death equations and to depict critics as ill-informed without basis only cheapens the debate.   The Met claims that the technology has a very low failure rate. However, using a different metric, last year research from the University of Essex said the tech only achieved eight correct matches out of 42, across six trials it evaluated. The latest algorithm used by the Met is said to show no bias on the base of ethnicity, although it is less accurate for women than men. Hannah Couchman, policy and campaigns officer at Liberty, says personal data is captured 'without consent' and 'undermines our rights to privacy'. She said: 'Anyone can be included on a facial recognition watch list - you do not have to be suspected of any wrongdoing, but could be put on a list for the ludicrously broad purpose of police 'intelligence interest'. 'Even if you're not on a watch list, your personal data is still being captured and processed without your consent - and often without you knowing it's happening. 'Facial recognition is a mass surveillance tool that undermines our rights to privacy and free expression. Any one of us might wish to go about our business anonymously and maintaining the right to do so does not make you worthy of police suspicion. 'The Met started using facial recognition after ignoring its own review of a two-year trial which found that its use of this technology had failed to meet human-rights requirements. 'By scaremongering and deriding criticisms rather than engaging with these concerns, Cressida Dick reveals how flimsy the basis for the Met's use of this oppressive tool really is.'   The Metropolitan Police is beginning operational use of facial recognition technology across London. It follows a number of trials of the cameras, which have been criticised by human rights campaigners as a risk to privacy. Here's how the technology works, and why it has proved so controversial. - How does it work? Live facial recognition (LFR) technology uses special cameras to scan the structure of faces in a crowd. The system then creates a digital image and compares the result against a 'watch list' made up of pictures of people who have been taken into police custody. Not everybody on police watch lists is wanted for the purposes of arrest - they can include missing people and other persons of interest. If a match is found, officers in the area of the cameras are alerted. - How much has it been used? The Met have used the technology multiple times since 2016, according to the force's website, including at Notting Hill Carnival in 2016 and 2017, Remembrance Day in 2017, and Port of Hull docks, assisting Humberside Police, in 2018. They have also undertaken several other trials in and around London since then. South Wales Police piloted the technology during the week of the 2017 Champions League final in Cardiff, the first UK force to use it at a large sporting event. Facial recognition has also been used at a number of privately-owned UK sites, including in shopping centres, museums and conference centres, according to an investigation by civil liberties group Big Brother Watch. - Why is it controversial? Campaigners say facial recognition breaches citizens' human rights. Liberty has said scanning and storing biometric data 'as we go about our lives is a gross violation of privacy'. Big Brother Watch says 'the notion of live facial recognition turning citizens into walking ID cards is chilling'. Some campaigners claim the technology will deter people from expressing views in public or going to peaceful protests. It is also claimed that facial recognition can be unreliable, and is least accurate when it attempts to identify black people, and women. In its own investigation into the technology, the Information Commissioner's Office (ICO) concluded that a legal code of practice should be introduced to ensure its safe deployment. In September last year, a High Court ruling said the use of the technology by South Wales Police had not been unlawful after an activist argued that having his face scanned caused him 'distress' and violated his privacy and data protection rights by processing an image taken of him in public. Ed Bridges, 36, from Cardiff, brought the challenge after claiming his face was scanned while he was doing his Christmas shopping in 2017 and at a peaceful anti-arms protest in 2018. After the ruling, Mr Bridges said he would appeal against the decision, which is due to be heard in June. - What do the police say? Speaking at the Met's announcement last month about the technology being rolled out, Assistant Commissioner Nick Ephgrave said the force is 'in the business of policing by consent' and thinks it is effectively balancing the right to privacy with crime prevention. He said: 'Everything we do in policing is a balance between common law powers to investigate and prevent crime, and Article 8 rights to privacy. 'It's not just in respect of live facial recognition, it's in respect of covert operations, stop and search - there's any number of examples where we have to balance individuals' right to privacy against our duty to prevent and deter crime.' Facial recognition software works by matching real time images to a previous photograph of a person.  Each face has approximately 80 unique nodal points across the eyes, nose, cheeks and mouth which distinguish one person from another.  A digital video camera measures the distance between various points on the human face, such as the width of the nose, depth of the eye sockets, distance between the eyes and shape of the jawline. This produces a unique numerical code that can then be linked with a matching code gleaned from a previous photograph. A facial recognition system used by officials in China connects to millions of CCTV cameras and uses artificial intelligence to pick out targets. Experts believe that facial recognition technology will soon overtake fingerprint technology as the most effective way to identify people.      
